"""
FastAPI application for MENA Bias Evaluation
Optimized for Hugging Face Spaces deployment
"""
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import os
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="MENA Bias Evaluation API",
    description="Enterprise-grade bias detection for Arabic/Persian NLP",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response Models
class PredictionRequest(BaseModel):
    text: str = Field(..., description="Text to analyze", min_length=1)
    language: Optional[str] = Field(None, description="Language code (ar/fa/en)")
    model_name: Optional[str] = Field(None, description="Model to use")

class PredictionResponse(BaseModel):
    text: str
    sentiment: str
    confidence: float
    language: Optional[str] = None
    bias_score: Optional[float] = None
    processing_time_ms: float
    timestamp: str

class HealthResponse(BaseModel):
    status: str
    timestamp: str
    version: str
    models_loaded: bool

class BatchPredictionRequest(BaseModel):
    texts: List[str] = Field(..., description="List of texts to analyze")
    language: Optional[str] = None
    model_name: Optional[str] = None

class BatchPredictionResponse(BaseModel):
    results: List[PredictionResponse]
    total_processed: int
    processing_time_ms: float

# Global variables
MODEL_LOADED = False
MODEL = None
TOKENIZER = None

@app.on_event("startup")
async def startup_event():
    """Load model on startup"""
    global MODEL_LOADED, MODEL, TOKENIZER
    try:
        logger.info("Loading model...")
        
        # Import here to avoid early import issues
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        import torch
        
        # Get model name from environment or use default
        model_name = os.getenv(
            "MODEL_NAME", 
            "CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment"
        )
        
        logger.info(f"Loading model: {model_name}")
        
        # Load tokenizer and model
        TOKENIZER = AutoTokenizer.from_pretrained(model_name)
        MODEL = AutoModelForSequenceClassification.from_pretrained(model_name)
        
        # Set to eval mode
        MODEL.eval()
        
        MODEL_LOADED = True
        logger.info("✅ Model loaded successfully!")
        
    except Exception as e:
        logger.error(f"❌ Failed to load model: {e}")
        MODEL_LOADED = False

@app.get("/", response_model=Dict[str, str])
async def root():
    """Root endpoint"""
    return {
        "message": "MENA Bias Evaluation API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    return HealthResponse(
        status="healthy" if MODEL_LOADED else "unhealthy",
        timestamp=datetime.now().isoformat(),
        version="1.0.0",
        models_loaded=MODEL_LOADED
    )

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """
    Predict sentiment for a single text
    
    Args:
        request: PredictionRequest with text and optional parameters
    
    Returns:
        PredictionResponse with sentiment analysis results
    """
    if not MODEL_LOADED:
        raise HTTPException(
            status_code=503,
            detail="Model not loaded. Please try again later."
        )
    
    try:
        import torch
        import time
        
        start_time = time.time()
        
        # Tokenize input
        inputs = TOKENIZER(
            request.text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding=True
        )
        
        # Run inference
        with torch.no_grad():
            outputs = MODEL(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
            predicted_class = torch.argmax(probs, dim=-1).item()
            confidence = probs[0][predicted_class].item()
        
        # Map prediction to sentiment
        sentiment_map = {0: "negative", 1: "neutral", 2: "positive"}
        sentiment = sentiment_map.get(predicted_class, "unknown")
        
        processing_time = (time.time() - start_time) * 1000
        
        return PredictionResponse(
            text=request.text,
            sentiment=sentiment,
            confidence=confidence,
            language=request.language,
            processing_time_ms=round(processing_time, 2),
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch_predict", response_model=BatchPredictionResponse)
async def batch_predict(request: BatchPredictionRequest):
    """
    Predict sentiment for multiple texts
    
    Args:
        request: BatchPredictionRequest with list of texts
    
    Returns:
        BatchPredictionResponse with list of predictions
    """
    if not MODEL_LOADED:
        raise HTTPException(
            status_code=503,
            detail="Model not loaded. Please try again later."
        )
    
    try:
        import time
        
        start_time = time.time()
        results = []
        
        for text in request.texts:
            pred_request = PredictionRequest(
                text=text,
                language=request.language,
                model_name=request.model_name
            )
            result = await predict(pred_request)
            results.append(result)
        
        processing_time = (time.time() - start_time) * 1000
        
        return BatchPredictionResponse(
            results=results,
            total_processed=len(results),
            processing_time_ms=round(processing_time, 2)
        )
        
    except Exception as e:
        logger.error(f"Batch prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def list_models():
    """List available models"""
    return {
        "available_models": [
            "CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment",
            "aubmindlab/bert-base-arabertv2",
            "asafaya/bert-base-arabic"
        ],
        "current_model": os.getenv(
            "MODEL_NAME",
            "CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment"
        )
    }

@app.get("/info")
async def get_info():
    """Get API information"""
    return {
        "name": "MENA Bias Evaluation API",
        "version": "1.0.0",
        "description": "Enterprise-grade bias detection for Arabic/Persian NLP",
        "endpoints": {
            "predict": "/predict",
            "batch_predict": "/batch_predict",
            "health": "/health",
            "models": "/models",
            "docs": "/docs"
        },
        "supported_languages": ["ar", "fa", "en"],
        "max_text_length": 512
    }

# Error handlers
@app.exception_handler(404)
async def not_found_handler(request, exc):
    return {
        "error": "Not Found",
        "message": "The requested endpoint does not exist",
        "available_endpoints": ["/", "/health", "/predict", "/batch_predict", "/models", "/info", "/docs"]
    }

@app.exception_handler(500)
async def internal_error_handler(request, exc):
    logger.error(f"Internal error: {exc}")
    return {
        "error": "Internal Server Error",
        "message": "An unexpected error occurred. Please try again later."
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)